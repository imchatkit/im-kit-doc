# IM技术详细方案

## 背景

本篇文章内容将从模型设计原理到具体的技术架构、最底层的存储模型，全方位设计企业级IM。



## 技术挑战

ImKit 有着丰富的功能，单聊、各种类型的群聊、消息已读、文字表情、多端同步、动态卡片、消息安全和存储等等。

**同时：**内部很多业务模块，比如文档、音视频、考勤、审批和CRM等，每个业务都在使用 IM，用于实现业务流程通知、运营消息推送、业务信令下发等。每个业务模块对于 IM 调用的流量峰值模型各有差别，对可用性要求也不尽相同。**IM 需要能够面对这些复杂的场景，保持良好的可用性和体验，同时兼顾性能与成本**。

通用的即时消息系统对消息发送的成功率、时延、到达率有很高的要求，企业 IM 由于 ToB 的特性，在数据安全可靠、系统可用性、多终端体验、开放定制等多个方面有着极致的要求。

**构建稳定高效的企业 IM 服务，ImKit 主要面临的挑战是：**

- ***1）***企业 IM 极致的体验要求对于系统架构设计的挑战：比如数据长期保存可漫游、多端数据同步、动态消息等带来的数据存储效率和成本压力，多端数据同步带来的一致性问题等；
- ***2）***极限场景冲击、依赖系统错误带来的可用性问题：比如超大群消息，突发疫情带来的线上办公和线上教学高并发流量，系统需要能够应对流量的冲击，保障高可用；同时在中间件普遍可用性不到 99.99% 的时候，IM 服务需要保障核心功能的 99.995% 的可用性；

**IM 在系统设计上：**

- ***1）***为了实现消息收发体验、性能和成本的平衡，设计了高效的读写扩散模型和同步服务，以及 NoSQL 存储，前期可以先使用MySql，但要做好无缝迁移NoSql的KV存储来做冷备或降低存储成本；
- ***2）***通过对 ImKit 服务流量的分析，对于大群消息、单账号大量的消息热点以及消息更新热点的场景进行了合并、削峰填谷等处理；
- ***3）***核心链路的应用中间件的依赖做容灾处理，实现了单一中间件失败不影响核心消息收发，保障基础的用户体验。

在消息存储过程中，一旦出现存储系统写入异常，系统会回旋缓冲重做，并且在服务恢复时，数据能主动向端上同步。

**在分层上遵从的原则为重云轻端：**业务数据计算、存储、同步等复杂操作尽量后移到云端处理，客户端只做终态数据的接收、展示，通过降低客户端业务实现的复杂度，最大化地提升客户端迭代速度，让端上开发可以专注于提升用户的交互体验，所有的功能需求和系统架构都围绕着该原则做设计和扩展。



## 模型设计

### 系统架构

低延迟、高触达、高可用是 ImKit 设计的第一原则，依据这个原则在架构上 ImKit 将系统拆分为三个服务做能力的承载。

**三个服务分别是：**

- ***1）***消息服务：负责 IM 核心消息模型和开放 API，IM 基础能力包括消息发送、单聊关系维护、群组元信息管理、历史消息拉取、已读状态通知、IM 数据存储以及跨地域的流量转发；
- ***2）***同步服务：负责用户消息数据以及状态数据的端到端同步，通过客户端到服务端长连接通道做实时的数据交互，当各类设备在线时 IM 及上游各业务通过同步服务做多端的数据同步，保障各端数据和体验一致；
- ***3）***通知服务：负责用户第三方通道维护以及通知功能，当的自建通道无法将数据同步到端上时，通过三方提供的通知和透传能力做消息推送，保障消息的及时性和有效性。

同步服务和通知服务除了服务于消息服务，也面向其他业务比如音视频、直播、Ding、文档等多端 (多设备) 数据同步。



### 消息收发链路



**消息发送：**消息发送接口由 Receiver 提供，统一接入层将用户从客户端发送的消息请求转发到 Receiver 模块，Receiver 校验消息的合法性（文字图片等安全审核、群禁言功能是否开启或者是否触发会话消息限流规则等）以及成员关系的有效性（单聊校验二者聊天、群聊校验发送者在群聊成员列表中），校验通过后为该消息生成一个全局唯一的 MessageId 随消息体以及接收者列表打包成消息数据包投递给异步队列，由下游 Processor 处理。消息投递成功之后，Receiver 返回消息发送成功的回执给客户端。

**消息处理 ：** Processor 消费到 IM 发送事件首先做按接收者用户的消息做本地存储入库（消息体、接收者维度、已读状态、个人会话列表红点更新），最后将消息体以及接收者列表打包为 IM 同步事件通过异步队列转发给同步服务。

**消息接收 ：**同步服务按接收者维度写入各自的同步队列，同时查取当前用户设备在线状态，当用户在线时捞取队列中未同步的消息，通过接入层长连接推送到各端。当用户离线时，打包消息数据以及离线用户状态列表为 IM 通知事件，转发给通知服务的 PNS 模块，PNS 查询离线设备做三方厂商通道推送，至此一条消息的推送流程结束。

### 存储模型设计

**了解 IM 服务最快的途径就是掌握它的存储模型。**

业界主流 IM 服务对于消息、会话、会话与消息的组织关系虽然不尽相同，但是归纳起来主要是两种形式：写扩散读聚合、读扩散写聚合。

所谓读写扩散其实是定义消息在群组会话中的存储形式。如下图所示。





**如上图所示：**



- ***1）***读扩散的场景：消息归属于会话，对应到存储中相当于有张 room_message 的表存储着该会话产生的所有消息 (rid->msgid->message，rid 房间 ID、msgid 消息 ID、message 消息)，这样实现的好处是消息入库效率高，只存储会话与消息的绑定关系即可；
- ***2）***写扩散的场景：会话产生的消息投递到类似于个人邮件的收件箱，即 message_inbox 表，存储个人的所有消息（uid->msgid->message， uid 用户 ID、msgid 消息 ID、message 消息），基于这种实现，会话中的每条消息面向不同的接收者可以呈现出不同状态。

**采用读扩散：**在个性化的消息扩展及实现层面有很大的约束， 例如消息撤回，修改，已读和未读状态，

**采用写扩散带来的问题也很明显：**一个群成员为 N 的会话一旦产生消息就会扩散 N 条消息记录，如果在消息发送和扩散量较少的场景，这样的实现相比于读扩散落地更为简单，存储成本也不是问题。但是 ImKit会话活跃度超高，一条消息的平均扩散比可以达到 1：30，超大群又是企业 IM 最核心的沟通场景，如果采用完全写扩散所带来存储成本问题势必制约业务发展。



### 同步模型设计



#### 推送模型

**用户在会话中发出的消息和消息状态变更等事件是如何同步到端上呢？**

**业界关于消息的同步模型的实现方案大致有三种：**



- ***1）***客户端拉取方案；
- ***2）***服务端推送方案；
- ***3）***服务端推送位点之后客户端拉取的推拉结合方案。

**三种方案各有优劣，在此简短总结：**



- ***1）***首先：客户端拉取方案的优点是该方案实施简单、研发成本低，是传统的 B/S 架构。劣势是效率低下，拉取间隔控制权在客户端，对于 IM 这种实时的场景，很难设置一个有效的拉取间隔，间隔太短对服务端压力大，间隔太长时效性差；
- ***2）***其次：服务端主动推送方案的优点是低延迟、能做到实时，最重要的主动权在服务端。劣势相对拉取方案，如何协调服务端和客户端的处理能力存在问题；
- ***3）***最后：推拉结合这个方案整合了拉和推的优点，但是方案更复杂，同时会比推的方案多一次 RTT，特别是在移动网络的场景下，不得不面临功耗和推送成功率的问题。

**ImKit相对传统 toC 的场景，有较明显的区别：**

***1）**  第一是对实时性的要求： 在企业服务中，比如员工聊天消息、各种系统报警，又比如音视频中的共享画板，无不要求实时事件同步，因此需要一种低延时的同步方案。

***2）**  第二是弱网接入的能力： 在 ImKit服务的对象中，上千万的企业组织涉及各行各业，从大城市 5G 的高速到偏远的山区弱网，都需要 ImKit的消息能发送、能触达。对于复杂的网络环境，需要服务端能判断接入环境，并依据不同的环境条件调整同步数据的策略。

***3）**  第三是功耗可控成本可控： 在 ImKit的企业场景中，消息收发频率比传统的 IM 多出一个数量级，在这么大的消息收发场景怎么保障 ImKit的功耗可控，特别是移动端的功耗可控，是 ImKit必须面对的问题。在这种要求下，就需要 ImKit尽量降低 IO 次数，并基于不同的消息优先级进行合并同步，既能要保障实时性不被破坏，又要做到低功耗。

**从以上三点可知，服务端主动推送的模型更适合 ImKit场景：**



- ***1）***首先可以做到极低的延时，保障推送耗时在毫秒级别；
- ***2）***其次是服务端能通过用户接入信息判断用户接入环境好坏，进行对应的分包优化，保障弱网链路下的成功率；
- ***3）***最后是主动推送相对于推拉结合来说，可以降低一次 IO，对 ImKit这种每分钟过亿消息服务来说，能极大的降低设备功耗，同时配合消息优先级合并包的优化，进一步降低端的功耗。

虽说主动推送有诸多优势，但是客户端会离线，甚至客户端处理速度无法跟上服务端的速度，必然导致消息堆积。

ImKit为了协调服务端和客户端处理能力不一致的问题，支持 Rebase 的能力，当服务端消息堆积的条数达到一定阈值时触发 Rebase，客户端会从 ImKit拉取最新的消息，同时服务端跳过这部分消息从最新的位点开始推送消息。ImKit称这个同步模型为推优先模型（Preferentially-Push Model，PPM）。﻿

在基于 PPM 的推送方案下，为了保障消息的可靠达到，ImKit还做一系列优化。

**这些优化具体是：**

- ***1）***支持消息可重入：服务端可能会针对某条消息做重复推送，客户端需要根据 msgId 做去重处理，避免端上消息重复展示。
- ***2）***支持消息排序：服务端推送消息特别是群比较活跃的场景，某条消息由于推送链路或者端侧网络抖动，推送失败，而新的消息正常推送到端侧，如果端上不做消息排序的话，消息列表就会发生乱序，所以服务端会为每条消息分配一个时间戳，客户端每次进入消息列表就是根据时间戳做排序再投递给 UI 层做消息展示。
- ***3）***支持缺失数据回补：在某个极端情况下客户端群消息事件比群创建事件更早到达端上，此时端上没有群的基本信息消息也就无法展现，所以需要客户端主动向服务端拉取群信息同步到本地，再做消息的透出。

#### 多端数据一致性

多端数据一致性问题一直是多端同步最核心的问题，单个用户可以同时在 PC、Pad 以及 Mobile 登录，消息、会话红点等状态需要在多端保持一致，并且用户更换设备情况下消息可以做全量的回溯。﻿

基于上面的业务诉求以及系统层面面临的诸多挑战，ImKit自研了同步服务来解决一致性问题。

**ImKit的同步服务的设计理念和原则如下：**

- ***1）***统一消息模型抽象，对于 ImKit服务产生的新消息以及已读事件、会话增删改、多端红点清除等事件统一抽象为同步服务的事件；
- ***2）***同步服务不感知事件的类型以及数据序列化方式。同步服务为每个用户的事件分配一个自增的 ID（注：这里非连续递增），确保消息可以根据 ID 做遍历的有序查询；
- ***3）***统一同步队列，同步服务为每个用户分配了一个 FIFO 的队列存储，自增 id 和事件串行写入队列；当有事件推送时，服务端根据用户当前各端在线设备状态做增量变更，将增量事件推送到各端；
- ***4）***根据设备和网络质量的不同可以做多种分包推送策略，确保消息可以有序、可靠、高效的发送给客户端。

**上面介绍了 ImKit的存储模型以及同步模型的设计与思考：**

- ***1）***在存储优化中，存储会基于 ImKit消息特点进行深度优化，并会对其中原理以及实现细节做深入分析与介绍；
- ***2）***在同步机制中，会进一步介绍多端同步机制是如何保障消息必达以及各端消息一致性。



# 多端同步机制设计

## 概述

ImKit面向办公场景，和面向普通用户的产品在服务端到客户端的数据同步上最大的区别是消息量巨大、变更事件复杂、对多端同步有着强烈的诉求。

ImKit基于同步服务构建了一套完整同步流程。同步服务是一个服务端到客户端的数据同步服务，是一套统一的数据下行平台，支撑ImKit多个应用服务。

**图6：**微服务架构 ▼





**同步服务是一套多端的数据同步服务，由两部分组成：**



- ***1）***部署于服务端的同步服务；
- ***2）***由客户端 APP 集成的同步服务 SDK。

**工作原理类似于MQ消息队列：**



- ***1）***用户 ID 近似消息队列中的 Topic；
- ***2）***用户设备近似消息队列中的 Consumer Group。

每个用户设备作为一个消息队列消费者能够按需获得这个用户一份数据拷贝，从而实现了多端同步诉求。

**当业务需要同步一个变更数据到指定的用户或设备时：**业务调用数据同步接口，服务端会将业务需要同步的数据持久化到存储系统中，然后当客户端在线的时候把数据推送给客户端。每一条数据入库时都会原子的生成一个按用户维度单调递增的位点，服务端会按照位点从小到大的顺序把每一条数据都推送至客户端。

**客户端应答接收成功后：**更新推送数据最大的位点到位点管理存储中，下次推送从这个新的位点开始推送。同步服务 SDK 内部负责接收同步服务数据，接收后写入本地数据库，然后再把数据异步分发到客户端业务模块，业务模块处理成功后删除本地存储对应的内容。

在上文章节中，已经初步介绍同步服务推送模型和多端一致性的考虑，本章主要是介绍 ImKit是如何做存储设计、在多端同步如何实现数据一致性、最后再介绍服务端消息数据堆积技术方案 Rebase。



## 全量消息存储逻辑

**在同步服务中，采用以用户为中心，将所有要推送给此用户的消息汇聚在一起，并为每个消息分配唯一且递增的 PTS**（即位点，英文术语Point To Sequence），服务端保存每个设备推送的位点。

通过两个用户 Bob 和 Alice，来实际展示消息在存储系统中存储的逻辑形态。例如：Bob 给 Alice 发送了一个消息”Hi! Alice“，Alice 回复了 Bob 消息”Hi! Bob“。

当 Bob 发送第一条消息给 Alice 时，接收方分别是 Bob 和 Alice，系统会在 Bob 和 Alice 的存储区域末尾分别添加一条消息，存储系统在入库成功时，会分别为这两行分配一个唯一且递增的位点（Bob 的位点是 10005，Alice 的位点是 23001）；入库成功之后，触发推送。比如 Bob 的 PC 端上一次下推的位点是 10000，Alice 移动端的推送位点是 23000，在推送流程发起之后，会有两个推送任务，第一是 Bob 的推送任务，推送任务从上一次位点（10000） + 1 开始查询数据，将获取到 10005 位置的”Hi“消息，将此消息推送给 Bob 的设备，推送成功之后，存储推送位点（10005）。Alice 推送流程也是同理。Alice 收到 Bob 消息之后，Alice 回复 Bob，类似上面的流程，入库成功并分配位点（Bob 的位点是 10009，Alice 的位点是 23003）。

**图7：**同步服务的存储设计﻿ ▼

![阿里IM技术分享(八)：深度解密ImKit即时消息服务DTIM的技术设计_7.png](D:\im项目资料\技术文档\doc\md\1.总体架构设计\图片\212707w5yvopp5vp4pdwbs.png)



## 消息多端同步逻辑

多端同步是 ImKit的典型特点，如何保持多端的数据及时触达和解决一致性是 ImKit同步服务最大的挑战。

上文中已经介绍了同步服务的事件存储模型，将需要推送的消息按照用户聚合。当用户有多个设备时，将设备的位点保存在位点管理系统之中，Key 是用户 + 设备 ID，Value 是上一次推送的位点。如果是设备第一次登录，位点默认为 0。

**由此可知：**每个设备都有单独的位点，**数据在服务端只有一份按照用户维度的数据**，推送到客户端的消息是服务端对应位点下的快照，从而保障了每个端的数据都是一致的。

**比如：**此时 Bob 登录了手机（该设备之前登录过ImKit），同步服务会获取到设备登录的事件，事件中有此设备上次接收数据的位点（比如 10000），同步服务会从 10000 + 1（位点）开始/查询数据，获取到五条消息（10005~10017），将消息推送给此台手机并更新服务端位点。此时，Bob 手机和 PC 上的消息一致，当 Alice 再次发送消息时，同步服务会给 Bob 的两台设备推送消息，始终保持 Bob 两个设备之间消息数据的一致性。

## 大量需同步离线消息的优化逻辑

**正如上文所述：**我们采用了推优先的模型下推数据以保障事件的实时性，采用位点管理实现多端同步，但是实际情况却远比上面的情况复杂。

**最常见的问题：**就是设备离线重新登录，期间该用户可能会累计大量未接收的消息数据，比如几万条。如果按照上面的方案，服务端在短时间会给客户端推送大量的消息，客户端 CPU 资源极有可能耗尽导致整个设备假死。

**其实对于 IM 这种场景来说：**几天甚至几小时之前的数据，再推送给用户已经丧失即时消息的意义，反而会消耗客户移动设备的电量，得不偿失。又或者节假日大群中各种活动，都会有大量的消息产生。

**对于以上情况：**同步服务提供 Rebase 的方案，当要推送的消息累计到一定阈值时，同步服务会向客户端发送 Rebase 事件，客户端收到事件之后，会从消息服务中获取到最新的消息（Lastmsg）。这样可以跳过中间大量的消息，当用户需要查看历史消息，可以基于 Lastmsg 向上回溯，即省电也能提升用户体验。

**还是以 Bob 为例：**Bob 登录了 Pad 设备（一台全新的设备），同步服务收到 Pad 登录的事件，发现登录的位点为 0，查询从 0 开始到当前，已经累计 1 万条消息，累计量大于同步服务的阈值，同步服务发送 Rebase 事件给客户端，客户端从消息服务中获取到最新的一条消息“Tks !!!”，同时客户端从同步服务中获取最新的位点为 10017，并告诉同步服务后续从 10017 这个位置之后开始推送。当 Bob 进入到和 Alice 的会话之后，客户端只要从 Lastmsg 向上回溯几条历史消息填满聊天框即可。